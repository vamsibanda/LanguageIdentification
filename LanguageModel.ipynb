{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import keras.optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 12\n",
    "else:\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES_DICT = {'en':0,'fr':1,'es':2,'it':3,'de':4,'sk':5,'cs':6}\n",
    "MAX_LEN = 140\n",
    "NUM_SAMPLES = 250000\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String of all characters from all above languages  \n",
      " abcdefghijklmnopqrstuvwxyzßàáâäæçèéêìíîïñòóôöùúûüýÿčďěĺľňœŕřšťůž !?¿¡ABCDEFGHIJKLMNOPQRSTUVWXYZÀÁÂÄÆÇÈÉÊÌÍÎÏÑÒÓÔÖÙÚÛÜÝČĎĚĹĽŇŒŔŘŠŤŮŸŽ\n",
      "Total number of characters  132\n"
     ]
    }
   ],
   "source": [
    "from support import define_alphabet\n",
    "\n",
    "alphabet = define_alphabet()\n",
    "print('String of all characters from all above languages ', '\\n', alphabet[2])\n",
    "\n",
    "VOCAB_SIZE = len(alphabet[2])\n",
    "print('Total number of characters ', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data\"\n",
    "source_directory = os.path.join(data_directory, 'source')\n",
    "cleaned_directory = os.path.join(data_directory, 'cleaned')\n",
    "samples_directory = os.path.join('/tmp', 'samples')\n",
    "train_test_directory = os.path.join('/tmp', 'train_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language :  en\n",
      "Content before cleaning :->   Mark's Eve. Those sitting had to keep silent between the bell tolling at 11.00 p.m. until the bell struck 1.00 a.m. In Yorkshire it was nec\n",
      "Content after cleaning :->  ing into the church. This practice took place throughout England, but was most prevalent in northern and western counties. Some accounts of \n",
      "Cleaning completed for : data/source/en.txt -> data/cleaned/en_cleaned.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Language :  fr\n",
      "Content before cleaning :->  ow Wilson.\n",
      "\n",
      "Au cours de sa carrière, il composa environ neuf cents toiles et plus de deux mille aquarelles, ainsi que d'innombrables croquis\n",
      "Content after cleaning :->  quarelles, ainsi que d'innombrables croquis et dessins. Son œuvre documente ses voyages à travers le monde, de Venise au Tyrol, de Corfou au\n",
      "Cleaning completed for : data/source/fr.txt -> data/cleaned/fr_cleaned.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Language :  es\n",
      "Content before cleaning :->  activos que pudiesen afectar a las personas. Después de que las instalaciones han sido completamente desmanteladas, estas ya no se encuentra\n",
      "Content after cleaning :->  a no se encuentran bajo la necesidad de control regulatorio de la autoridad respectiva y el concesionario de la central ya no es responsable\n",
      "Cleaning completed for : data/source/es.txt -> data/cleaned/es_cleaned.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Language :  it\n",
      "Content before cleaning :->  quali si comportano come \"ali rotanti\" che impongono una variazione di quantità di moto nella direzione della velocità di avanzamento, al co\n",
      "Content after cleaning :->  zione della velocità di avanzamento, al contrario delle ali convenzionali, la cui variazione di quantità di moto è normale alla direzione di\n",
      "Cleaning completed for : data/source/it.txt -> data/cleaned/it_cleaned.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Language :  de\n",
      "Content before cleaning :->  ält er den Auftrag zur Figur des David als Strebepfeilerbekrönung für den Dom, heute im Museo nazionale del Bargello. \n",
      "1414 beginnt er mit d\n",
      "Content after cleaning :->  m Museo nazionale del Bargello. 1414 beginnt er mit den ersten Figuren für die noch nicht bestückten Nischen des Campanile von Giotto. Seine\n",
      "Cleaning completed for : data/source/de.txt -> data/cleaned/de_cleaned.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Language :  sk\n",
      "Content before cleaning :->  ie kvalitných materiálov. Z rozsiahlych spoločenských priestorov sa v pôvodnej realizácii zachovala len divadelná sála, ostatné boli v päťde\n",
      "Content after cleaning :->  vadelná sála, ostatné boli v päťdesiatych rokoch, na škodu veci, preadaptované. „Bola to na svoju dobu (1924) pre mňa úloha veľká, zložitá, \n",
      "Cleaning completed for : data/source/sk.txt -> data/cleaned/sk_cleaned.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Language :  cs\n",
      "Content before cleaning :->  á žádnou souvislost). Rovněž původní prostřední jméno Marvolo má zjevně souvislost s anglickým slovem \"marvel\" - div, zázrak. \n",
      "\n",
      "Gauntovi jso\n",
      "Content after cleaning :->  ickým slovem \"marvel\" - div, zázrak. Gauntovi jsou čistokrevná kouzelnická rodina, potomci jednoho ze zakladatelů Školy čar a kouzel v Brada\n",
      "Cleaning completed for : data/source/cs.txt -> data/cleaned/cs_cleaned.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "END OF CLEANING\n"
     ]
    }
   ],
   "source": [
    "from support import clean_text\n",
    "\n",
    "for lang_code in LANGUAGES_DICT:\n",
    "\n",
    "    path_src = os.path.join(source_directory, lang_code+\".txt\")\n",
    "    with open(path_src) as source_file:\n",
    "\n",
    "        content = source_file.read()\n",
    "\n",
    "        print('Language : ',lang_code)\n",
    "        print('Content before cleaning :-> ', content[1000:1000+MAX_LEN])\n",
    "\n",
    "    content = clean_text(content)\n",
    "    \n",
    "    print ('Content after cleaning :-> ', content[1000:1000+MAX_LEN])\n",
    "    \n",
    "    path_cl = os.path.join(cleaned_directory, lang_code + '_cleaned.txt')\n",
    "    with open(path_cl,'w') as cleaned_file:\n",
    "        cleaned_file.write(content)\n",
    "    \n",
    "    # Free the memory\n",
    "    del content\n",
    "    print (\"Cleaning completed for : \" + path_src,'->',path_cl)\n",
    "    print (100*'-')\n",
    "print (\"END OF CLEANING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. SAMPLE TEXT: \n",
      " small tied island connected by the largest tombolo in the UK to the south-western coast of the Mainland, Shetland, in Scotland. It is part\n",
      "\n",
      "2. REFERENCE ALPHABET: \n",
      " ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ß', 'à', 'á', 'â', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'ì', 'í', 'î', 'ï', 'ñ', 'ò', 'ó', 'ô', 'ö', 'ù', 'ú', 'û', 'ü', 'ý', 'ÿ', 'č', 'ď', 'ě', 'ĺ', 'ľ', 'ň', 'œ', 'ŕ', 'ř', 'š', 'ť', 'ů', 'ž', ' ', '!', '?', '¿', '¡', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'À', 'Á', 'Â', 'Ä', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ì', 'Í', 'Î', 'Ï', 'Ñ', 'Ò', 'Ó', 'Ô', 'Ö', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Č', 'Ď', 'Ě', 'Ĺ', 'Ľ', 'Ň', 'Œ', 'Ŕ', 'Ř', 'Š', 'Ť', 'Ů', 'Ÿ', 'Ž']\n",
      "\n",
      "3. SAMPLE INPUT ROW: \n",
      " [9, 2, 4, 6, 11, 1, 1, 6, 7, 0, 1, 8, 3, 10, 9, 1, 0, 3, 9, 16, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "4. INPUT SIZE (VOCAB SIZE):  132\n",
      "\n",
      "4. Associated character count: \n",
      " {'a': 9, 'b': 2, 'c': 4, 'd': 6, 'e': 11, 'f': 1, 'g': 1, 'h': 6, 'i': 7, 'k': 1, 'l': 8, 'm': 3, 'n': 10, 'o': 9, 'p': 1, 'r': 3, 's': 9, 't': 16, 'u': 2, 'w': 1, 'y': 1, ' ': 23, 'I': 1, 'K': 1, 'M': 1, 'S': 2, 'U': 1}\n"
     ]
    }
   ],
   "source": [
    "from support import get_sample_text, get_input_row\n",
    "    \n",
    "path = os.path.join(cleaned_directory, \"en_cleaned.txt\")\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    \n",
    "    content = f.read()\n",
    "    random_index = random.randrange(0,len(content)-2*MAX_LEN)\n",
    "    sample_text = get_sample_text(content,random_index,MAX_LEN)\n",
    "    print (\"1. SAMPLE TEXT: \\n\", sample_text)\n",
    "    \n",
    "    all_characters = alphabet[0]+alphabet[1]\n",
    "    print (\"\\n2. REFERENCE ALPHABET: \\n\", all_characters)\n",
    "    \n",
    "    sample_input_row = get_input_row(content, random_index, MAX_LEN, alphabet)\n",
    "    print (\"\\n3. SAMPLE INPUT ROW: \\n\",sample_input_row)\n",
    "    \n",
    "    input_size = len(sample_input_row)\n",
    "    if input_size != VOCAB_SIZE:\n",
    "        print(\"Something strange happened!\")\n",
    "        \n",
    "    print (\"\\n4. INPUT SIZE (VOCAB SIZE): \", input_size)\n",
    "    \n",
    "    count_characters = {}\n",
    "    for i in range(VOCAB_SIZE):\n",
    "        if sample_input_row[i] != 0:\n",
    "           count_characters[all_characters[i]] = sample_input_row[i]\n",
    "    \n",
    "    print (\"\\n4. Associated character count: \\n\",count_characters)\n",
    "    del content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file : data/cleaned/en_cleaned.txt\n",
      "content_lenght 101420888, remaining_length 66420888, jump 199\n",
      "File size :  101.42 MB  | # possible samples :  768340 | # skip chars : 199\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : data/cleaned/fr_cleaned.txt\n",
      "content_lenght 98722777, remaining_length 63722777, jump 191\n",
      "File size :  98.72 MB  | # possible samples :  747899 | # skip chars : 191\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : data/cleaned/es_cleaned.txt\n",
      "content_lenght 97562749, remaining_length 62562749, jump 187\n",
      "File size :  97.56 MB  | # possible samples :  739111 | # skip chars : 187\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : data/cleaned/it_cleaned.txt\n",
      "content_lenght 101889621, remaining_length 66889621, jump 200\n",
      "File size :  101.89 MB  | # possible samples :  771891 | # skip chars : 200\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : data/cleaned/de_cleaned.txt\n",
      "content_lenght 101217249, remaining_length 66217249, jump 198\n",
      "File size :  101.22 MB  | # possible samples :  766797 | # skip chars : 198\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : data/cleaned/sk_cleaned.txt\n",
      "content_lenght 85647924, remaining_length 50647924, jump 151\n",
      "File size :  85.65 MB  | # possible samples :  648847 | # skip chars : 151\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : data/cleaned/cs_cleaned.txt\n",
      "content_lenght 90564992, remaining_length 55564992, jump 166\n",
      "File size :  90.56 MB  | # possible samples :  686098 | # skip chars : 166\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vocab Size :  132\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Samples array size :  (1750000, 133)\n",
      "/tmp/samples/lang_samples_132.npz size :  57.21 MB\n"
     ]
    }
   ],
   "source": [
    "def size_mb(size):\n",
    "    size_mb =  '{:.2f}'.format(size/(1000*1000.0))\n",
    "    return size_mb + \" MB\"\n",
    "\n",
    "\n",
    "sample_data = np.empty((NUM_SAMPLES*len(LANGUAGES_DICT),VOCAB_SIZE+1),dtype = np.uint16)\n",
    "lang_seq = 0\n",
    "jump_reduce = 0.2 # part of characters removed from jump to avoid passing the end of file\n",
    "\n",
    "for lang_code in LANGUAGES_DICT:\n",
    "    start_index = 0\n",
    "    path = os.path.join(cleaned_directory, lang_code+\"_cleaned.txt\")\n",
    "    with open(path, 'r') as f:\n",
    "        print (\"Processing file : \" + path)\n",
    "        file_content = f.read()\n",
    "        content_length = len(file_content)\n",
    "        remaining = content_length - MAX_LEN*NUM_SAMPLES\n",
    "        jump = int(((remaining/NUM_SAMPLES)*3)/4)\n",
    "        print('content_lenght {}, remaining_length {}, jump {}'.format(content_length,remaining,jump))\n",
    "        print (\"File size : \",size_mb(content_length),\\\n",
    "               \" | # possible samples : \",int(content_length/VOCAB_SIZE),\\\n",
    "              \"| # skip chars : \" + str(jump))\n",
    "        for idx in range(NUM_SAMPLES):\n",
    "            input_row = get_input_row(file_content, start_index, MAX_LEN, alphabet)\n",
    "            sample_data[NUM_SAMPLES*lang_seq+idx,] = input_row + [LANGUAGES_DICT[lang_code]]\n",
    "            start_index += MAX_LEN + jump\n",
    "        del file_content\n",
    "    lang_seq += 1\n",
    "    print (100*\"-\")\n",
    "\n",
    "np.random.shuffle(sample_data)\n",
    "\n",
    "print (\"Vocab Size : \",VOCAB_SIZE )\n",
    "print (100*\"-\")\n",
    "print (\"Samples array size : \",sample_data.shape )\n",
    "\n",
    "if not os.path.exists(samples_directory):\n",
    "    os.makedirs(samples_directory)\n",
    "\n",
    "path_smpl = os.path.join(samples_directory,\"lang_samples_\"+str(VOCAB_SIZE)+\".npz\")\n",
    "np.savez_compressed(path_smpl,data=sample_data)\n",
    "print(path_smpl, \"size : \", size_mb(os.path.getsize(path_smpl)))\n",
    "del sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.load(path_smpl)['data']\n",
    "dt = dt.astype(np.float32)\n",
    "X = dt[:, 0:input_size]\n",
    "Y = dt[:, input_size]\n",
    "del dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = standard_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = keras.utils.to_categorical(Y, num_classes=len(LANGUAGES_DICT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=SEED)\n",
    "del X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/train_test/train_test_data_132.npz size :  94.93 MB\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(train_test_directory):\n",
    "    os.makedirs(train_test_directory)\n",
    "\n",
    "path_tt = os.path.join(train_test_directory,\"train_test_data_\"+str(VOCAB_SIZE)+\".npz\")\n",
    "np.savez_compressed(path_tt,X_train=X_train,Y_train=Y_train,X_test=X_test,Y_test=Y_test)\n",
    "print(path_tt, \"size : \",size_mb(os.path.getsize(path_tt)))\n",
    "del X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (1400000, 132)\n",
      "Y_train:  (1400000, 7)\n",
      "X_test:  (350000, 132)\n",
      "Y_test:  (350000, 7)\n"
     ]
    }
   ],
   "source": [
    "path_tt = os.path.join(train_test_directory, \"train_test_data_\"+str(VOCAB_SIZE)+\".npz\")\n",
    "train_test_data = np.load(path_tt)\n",
    "\n",
    "# Train Set\n",
    "X_train = train_test_data['X_train']\n",
    "print (\"X_train: \",X_train.shape)\n",
    "Y_train = train_test_data['Y_train']\n",
    "print (\"Y_train: \",Y_train.shape)\n",
    "\n",
    "# Test Set\n",
    "X_test = train_test_data['X_test']\n",
    "print (\"X_test: \",X_test.shape)\n",
    "Y_test = train_test_data['Y_test']\n",
    "print (\"Y_test: \",Y_test.shape)\n",
    "\n",
    "del train_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 500)               66500     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 247,607\n",
      "Trainable params: 247,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(500,input_dim=input_size, kernel_initializer=\"glorot_uniform\", activation=\"tanh\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(300, kernel_initializer=\"glorot_uniform\", activation=\"tanh\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, kernel_initializer=\"glorot_uniform\", activation=\"tanh\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(LANGUAGES_DICT), kernel_initializer=\"glorot_uniform\", activation=\"softmax\"))\n",
    "model_optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=model_optimizer,metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1260000 samples, validate on 140000 samples\n",
      "Epoch 1/5\n",
      " - 194s - loss: 0.1266 - acc: 0.9621 - val_loss: 0.0953 - val_acc: 0.9708\n",
      "Epoch 2/5\n",
      " - 614s - loss: 0.1128 - acc: 0.9665 - val_loss: 0.0894 - val_acc: 0.9725\n",
      "Epoch 3/5\n",
      " - 664s - loss: 0.1104 - acc: 0.9671 - val_loss: 0.0874 - val_acc: 0.9733\n",
      "Epoch 4/5\n",
      " - 215s - loss: 0.1092 - acc: 0.9675 - val_loss: 0.0886 - val_acc: 0.9732\n",
      "Epoch 5/5\n",
      " - 210s - loss: 0.1087 - acc: 0.9678 - val_loss: 0.0878 - val_acc: 0.9736\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Tensorboard\n",
    "tensorboard = TensorBoard(log_dir=\"run\")\n",
    "\n",
    "history = model.fit(X_train,Y_train,epochs=EPOCHS,validation_split=0.1,batch_size=BATCH_SIZE,callbacks=[tensorboard],shuffle=True,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350000/350000 [==============================] - 18s 51us/step\n",
      "acc: 97.32%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict_classes(X_test)\n",
    "Y_pred = keras.utils.to_categorical(Y_pred, num_classes=len(LANGUAGES_DICT))\n",
    "LABELS =  list(LANGUAGES_DICT.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         en       0.96      0.97      0.97     49866\n",
      "         fr       0.98      0.98      0.98     49871\n",
      "         es       0.97      0.97      0.97     49995\n",
      "         it       0.97      0.97      0.97     49909\n",
      "         de       0.98      0.98      0.98     50364\n",
      "         sk       0.97      0.98      0.97     49923\n",
      "         cs       0.98      0.97      0.98     50072\n",
      "\n",
      "avg / total       0.97      0.97      0.97    350000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_pred, target_names=LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('data/models/lang_identification_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a790acb79e9d4513bb5668d1ffaad496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value='', description='TEXT', placeholder='Type the text to identify here'), Bu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact_manual\n",
    "from ipywidgets import widgets\n",
    "from support import clean_text\n",
    "\n",
    "\n",
    "def get_prediction(TEXT):\n",
    "    if len(TEXT) < MAX_LEN:\n",
    "        print(\"Text has to be at least {} chars long, but it is {}/{}\".format(MAX_LEN, len(TEXT), MAX_LEN))\n",
    "        return(-1)\n",
    "    # Data cleaning\n",
    "    cleaned_text = clean_text(TEXT)\n",
    "    \n",
    "    # Get the MAX_LEN char\n",
    "    input_row = get_input_row(cleaned_text, 0, MAX_LEN, alphabet)\n",
    "    \n",
    "    # Data preprocessing (Standardization)\n",
    "    test_array = standard_scaler.transform([input_row])\n",
    "    \n",
    "    raw_score = model.predict(test_array)\n",
    "    pred_idx= np.argmax(raw_score, axis=1)[0]\n",
    "    score = raw_score[0][pred_idx]*100\n",
    "    \n",
    "    # Prediction\n",
    "    prediction = LABELS[model.predict_classes(test_array)[0]]\n",
    "    print('TEXT:', TEXT, '\\nPREDICTION:', prediction.upper(), '\\nSCORE:', score)\n",
    "\n",
    "interact_manual(get_prediction, TEXT=widgets.Textarea(placeholder='Type the text to identify here'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
